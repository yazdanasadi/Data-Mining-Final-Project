{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Final Project\n",
    "### Yazdan asadi \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import walk\n",
    "import math \n",
    "import random\n",
    "import sets\n",
    "import time\n",
    "from IPython.display import Image\n",
    "import uuid\n",
    "import pydot\n",
    "from sklearn.model_selection import train_test_split\n",
    "import graphviz \n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import six\n",
    "from random import randrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/yazdan/Desktop/DM_Project'\n",
    "df_dic = {}\n",
    "for root, dirs, files in walk(path):\n",
    "    for file in files:\n",
    "        if root not in df_dic:\n",
    "            df_dic[root] = [file]\n",
    "        else:\n",
    "            df_dic[root].append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after we put all subfolders in the dictionary we choose to get datas in each folder to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for i in df_dic:\n",
    "    for f in df_dic[i]:\n",
    "        if f.split('.')[-1] in ['trn','tst','data']:\n",
    "            full_path = i + '/' + f\n",
    "            dfs.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/yazdan/Desktop/DM_Project/1-Balance Scale Database/balance-scale.data',\n",
       " '/home/yazdan/Desktop/DM_Project/2-Balloons Database/yellow-small.data',\n",
       " '/home/yazdan/Desktop/DM_Project/2-Balloons Database/adult+stretch.data',\n",
       " '/home/yazdan/Desktop/DM_Project/2-Balloons Database/adult-stretch.data',\n",
       " '/home/yazdan/Desktop/DM_Project/2-Balloons Database/yellow-small+adult-stretch.data',\n",
       " '/home/yazdan/Desktop/DM_Project/8-Adult Database/adult.data',\n",
       " '/home/yazdan/Desktop/DM_Project/7-Event Detection Database/CalIt2.data',\n",
       " '/home/yazdan/Desktop/DM_Project/7-Event Detection Database/Dodgers.data',\n",
       " '/home/yazdan/Desktop/DM_Project/6-Dermatology Database/dermatology.data',\n",
       " '/home/yazdan/Desktop/DM_Project/9-Abalone Database/abalone.data',\n",
       " '/home/yazdan/Desktop/DM_Project/4-Credit Screening Databases/crx.data',\n",
       " '/home/yazdan/Desktop/DM_Project/5-Cylinder Bands Database/bands.data',\n",
       " '/home/yazdan/Desktop/DM_Project/3-Wisconsin Breast Cancer Databases/wpbc.data',\n",
       " '/home/yazdan/Desktop/DM_Project/3-Wisconsin Breast Cancer Databases/wdbc.data',\n",
       " '/home/yazdan/Desktop/DM_Project/3-Wisconsin Breast Cancer Databases/breast-cancer-wisconsin.data',\n",
       " '/home/yazdan/Desktop/DM_Project/10-Satimage/sat.tst',\n",
       " '/home/yazdan/Desktop/DM_Project/10-Satimage/sat.trn']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baloons_df_paths = dfs[1] +','+ dfs[2] +','+ dfs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_df_path = dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_df_paths = dfs[12] +','+ dfs[13] + ',' +dfs[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crx_df_path = dfs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satimage_df_paths = dfs[15] +','+ dfs[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_df_path = dfs[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df_path = dfs[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we found 7 datasets including test and train csv files. its time to read this csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "df = []\n",
    "data_to_csv = balance_df_path #enter your desire dataset path\n",
    "datas = data_to_csv.split(\",\")\n",
    "for i in datas:\n",
    "    df = pd.read_csv(i,sep=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDA and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1: Base on the problem and dataset we can drop the unnecessary columns!\n",
    "For each dataset i made a \"to_drop\" list for droping the cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [] #FILL THE LIST WITH COLS THAT YOU WANT!\n",
    "class_labels = ['Class-Name']\n",
    "df.drop(to_drop, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2: Checking if the class label(s) is unique or not! and checking the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class-Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = df.columns\n",
    "for i in col_names:\n",
    "      print('{} is unique: {}'.format(i, df[i].is_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 3: Checking the missing values and number of dirty values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before (shape of df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after (shape of df)\n",
    "rows_without_missing_data = df.dropna()\n",
    "rows_without_missing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_missing_columns = df.dropna(axis=1)\n",
    "data_without_missing_columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with ' '\n",
    "#df['col'] = df['col'].fillna(' ')\n",
    "# Fill NaN with 99\n",
    "#df['col'] = df['col'].fillna(99)\n",
    "# Fill NaN with the mean of the column\n",
    "#df['col'] = df['col'].fillna(df['col'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4: time to look at the information and describtion of our dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 5: Replacing the Classes with numbers and then we have label (y) for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_num = {'B': 0,\n",
    "                  'R': 1,\n",
    "                  'L': 2}\n",
    "df['tmp'] = df['Class-Name'].map(class_to_num)\n",
    "y = df['tmp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For checking if our data is imbalance or not we can use shannon entroy equation:                              \n",
    "0 for a unbalanced data set                                             \n",
    "1 for a balanced data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_series = pd.Series(df.all)\n",
    "counts = pd_series.value_counts()\n",
    "entropy = stats.entropy(counts)\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Train/Test Split without sklearn and Producing Decision Tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2.drop(class_labels,inplace=True, axis=1)\n",
    "np.random.seed(43)\n",
    "indices = list(range(df2.shape[0]))\n",
    "num_training_indices = int(0.66 * df2.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:num_training_indices]\n",
    "test_indices = indices[num_training_indices:]\n",
    "# split the actual data\n",
    "x_train, x_test = df2.iloc[train_indices], df2.iloc[test_indices]\n",
    "y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *time to make our decision tree*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.compute the entropy for data-set                      \n",
    "2.for every attribute/feature:\n",
    "                  1.calculate entropy for all categorical values\n",
    "               2.take average information entropy for the current attribute\n",
    "         3.calculate gain for the current attribute\n",
    "3. pick the highest gain attribute.\n",
    "4. Repeat until we get the tree we desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data,y_field,level=0,tree=None,verbose=False):\n",
    "    cols = list(data.columns)\n",
    "    cols.remove(y_field)\n",
    "    info_gains = []\n",
    "    for col in cols:\n",
    "        info_gains.append(round(info_gain(data,col,y_field),2))\n",
    "    if verbose:\n",
    "        print(cols,info_gains)\n",
    "    if(len(set(info_gains))==1): \n",
    "        col_to_split_on = random.choice(cols)        \n",
    "        values_to_split_on = data[col_to_split_on].unique()\n",
    "        node = col_to_split_on\n",
    "        if tree is None:\n",
    "            tree = {}\n",
    "            tree[node] = {}\n",
    "        for value in values_to_split_on:\n",
    "            sub_df = data[data[col_to_split_on]==value]\n",
    "            if verbose:\n",
    "                print(f'No info gain, uniques: {sub_df[y_field].unique()}')\n",
    "            most_frequent = sub_df[y_field].mode()[0]\n",
    "            if verbose:\n",
    "                print(f'Setting {col_to_split_on} with value {value} as leaf with output {most_frequent}')\n",
    "            tree[node][str(value)] = str(most_frequent)\n",
    "    else:\n",
    "        col_to_split_on = cols[info_gains.index(max(info_gains))]\n",
    "        values_to_split_on = data[col_to_split_on].unique()\n",
    "        node = col_to_split_on\n",
    "        if tree is None:\n",
    "            tree = {}\n",
    "            tree[node] = {}\n",
    "        for value in values_to_split_on:\n",
    "            sub_df = data[data[col_to_split_on]==value]\n",
    "            if verbose:\n",
    "                print(f'Level->{level},total_cols:{cols}, Split on {col_to_split_on} on value {value}, uniques:{sub_df[y_field].unique()}')\n",
    "            if len(sub_df[y_field].unique())==1:\n",
    "                tree[node][str(value)] = str(sub_df[y_field].unique()[0])\n",
    "                if verbose:\n",
    "                    print(f'Level->{level}, Reached leaf for {col_to_split_on} at value {value} with output {sub_df[y_field].unique()[0]}')\n",
    "            else:\n",
    "                x = 'y'\n",
    "                if x !='n':\n",
    "                    tree[node][str(value)] = build_tree(sub_df,y_field,level=level+1,verbose=verbose)\n",
    "                else:\n",
    "                    tree[node][str(value)]=None\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data,y_field):\n",
    "    sum = 0\n",
    "    for el in data[y_field].value_counts():\n",
    "        prob = el/len(data)\n",
    "        sum += prob*math.log(prob,2)\n",
    "    sum*=-1\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(data,attribute,y_field):\n",
    "    current_entropy = entropy(data,y_field)\n",
    "    uniques = data[attribute].value_counts().keys()\n",
    "    sec_term = 0\n",
    "    for i,cat_count in enumerate(data[attribute].value_counts()):\n",
    "        sec_term += (cat_count/len(data))*entropy(data[data[attribute]==uniques[i]],y_field)\n",
    "    entropy_for_attribute = current_entropy - sec_term\n",
    "    return entropy_for_attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree = build_tree(x_train,'tmp',verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_dictionaryv2(graph, dictionary, parent_node=None):\n",
    "    for k in dictionary.keys():\n",
    "\n",
    "        if parent_node is not None:\n",
    "\n",
    "            from_name = parent_node.get_name().replace(\"\\\"\", \"\") + '_' + str(k)\n",
    "            from_label = str(k)\n",
    "            obj_dict = {}\n",
    "\n",
    "            if 'True' in from_label:\n",
    "                node_from = pydot.Node(from_name, color='white', style='filled',fillcolor='green', label=from_label)\n",
    "            elif 'False' in from_label:\n",
    "                node_from = pydot.Node(from_name, color='white', style='filled',fillcolor='red', label=from_label)\n",
    "            else:\n",
    "                node_from = pydot.Node(from_name, label=from_label)\n",
    "\n",
    "            graph.add_node(node_from)\n",
    "\n",
    "            graph.add_edge( pydot.Edge(parent_node, node_from) )\n",
    "\n",
    "            if isinstance(dictionary[k], dict): # if interim node\n",
    "                walk_dictionaryv2(graph, dictionary[k], node_from)\n",
    "\n",
    "            else: # if leaf node\n",
    "                to_name = str(uuid.uuid4()) + '_' + str(dictionary[k]) # unique name\n",
    "                to_label = str(dictionary[k])\n",
    "\n",
    "                node_to = pydot.Node(to_name, label=to_label, shape='box')\n",
    "                graph.add_node(node_to)\n",
    "                graph.add_edge(pydot.Edge(node_from, node_to))\n",
    "        else:\n",
    "\n",
    "            from_name =  str(k)\n",
    "            from_label = str(k)\n",
    "\n",
    "            node_from = pydot.Node(from_name, label=from_label)\n",
    "            walk_dictionaryv2(graph, dictionary[k], node_from)\n",
    "\n",
    "\n",
    "def plot_tree(tree, name):\n",
    "    graph = pydot.Dot(graph_type='graph')\n",
    "    walk_dictionaryv2(graph, tree)\n",
    "    graph.write_png(name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(my_tree,'first')\n",
    "Image(filename='first.png',unconfined=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the model with sklearn classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ent = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=0)\n",
    "# fit the model\n",
    "clf_ent.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ent = clf_ent.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_ent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8 : APRIOR / FP-Growth for freq-itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as itert\n",
    "from itertools import combinations\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "df = []\n",
    "data_to_csv = adult_df_path #enter your desire dataset path\n",
    "datas = data_to_csv.split(\",\")\n",
    "for i in datas:\n",
    "    df = pd.read_csv(i,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(df):\n",
    "    \n",
    "    # MISSING VALUES\n",
    "    df = df.applymap(lambda d: np.nan if d== ' ?' else d)\n",
    "    df = df.dropna(axis = 0) # Drop Rows With NaNs\n",
    "    \n",
    "    # DROP UNNESSESARY COLS\n",
    "    df.drop('fnlwgt', axis = 1, inplace = True)\n",
    "    df.drop('educational-num', axis = 1, inplace = True)\n",
    "    \n",
    "    # Converting Continuous Variables to Categorical\n",
    "    df['age'] = pd.cut(df['age'], [0, 30, 60, 100], \n",
    "                labels = ['Young', 'Middle-aged', 'Senior'], \n",
    "                right = True, include_lowest = True)\n",
    "    df['hours-per-week'] = pd.cut(df['hours-per-week'], [0, 20, 40, 100], \n",
    "                           labels = ['Part-time', 'Full-time', 'Over-time'], \n",
    "                           right = True, include_lowest = True)\n",
    "    df['capital-gain'] = pd.cut(df['capital-gain'], [0, 1, 100000], \n",
    "                       labels = ['No-Gain', 'Positive-Gain'], \n",
    "                       right = True, include_lowest = True)\n",
    "    df['capital-loss'] = pd.cut(df['capital-loss'], [0, 1, 100000],\n",
    "                   labels = ['No-Loss', 'Positive-Loss'], \n",
    "                   right = True, include_lowest = True)\n",
    "    \n",
    "    # Removing Spaces from the Data Entries\n",
    "    df = df.applymap(lambda x: x.strip() if type(x) is str else x)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(df, min_sup):\n",
    "    min_sup = (min_sup / 100.0) * len(df.index)\n",
    "    print('Minimum Support Count: ', int(min_sup))\n",
    "    \n",
    "    # Generate Initial Set of Candidates\n",
    "    print('\\nPlease Wait. Calculating Frequent Itemsets.')\n",
    "    candidates = getC1(df)\n",
    "    \n",
    "    # Generate Initial Set of Frequent Itemsets\n",
    "    frequent = {k:v for k, v in list(candidates.items()) if v >= min_sup}\n",
    "    freq = len(frequent) # No. of Frequent Itemsets Generated\n",
    "    step = 1\n",
    "    total = freq\n",
    "    display(frequent, step)\n",
    "    \n",
    "    while 1:\n",
    "        step += 1\n",
    "        print('\\n Calculating Frequent Itemsets.')\n",
    "        candidates = join(frequent, step)\n",
    "        frequent = prune(df, candidates, frequent, min_sup)\n",
    "        freq = len(frequent)\n",
    "        total += freq\n",
    "        if freq == 0:\n",
    "            break\n",
    "        display(frequent, step)\n",
    "        \n",
    "    print('\\nFrequent Itemsets Have Been Generated!')\n",
    "    print('No. of Frequent Itemsets Generated: ', total)\n",
    "    \n",
    "def getC1(df):\n",
    "    candidates = {}\n",
    "    for index, row in df.iterrows():\n",
    "        data = list(row)\n",
    "        for item in data:\n",
    "            if item in list(candidates.keys()):\n",
    "                candidates[item] += 1\n",
    "            else:\n",
    "                candidates[item] = 1\n",
    "    return candidates\n",
    "        \n",
    "def display(frequent, k):\n",
    "    print('\\n---- Frequent [', k, '] Itemsets ----')\n",
    "    for i in frequent:\n",
    "        key = i\n",
    "        val = frequent[i]\n",
    "        print(key, ' - ', val)\n",
    "        \n",
    "def join(frequent, k):\n",
    "    cand_sets = []\n",
    "    items = list(frequent.keys())\n",
    "    size = len(items)\n",
    "    for i in range(size):\n",
    "        for j in range(i, size):\n",
    "            joined = list(set(str(items[i]).split(' ')) | set(str(items[j]).split(' ')))\n",
    "            itemset = list(itert.combinations(joined, k))\n",
    "            if len(itemset) > 0:\n",
    "                itemset = list(itemset[0])\n",
    "                key = ' '.join(str(x) for x in itemset)\n",
    "                cand_sets.append(key)\n",
    "    return cand_sets\n",
    "\n",
    "def sortInto(candidates):\n",
    "    localCache = []\n",
    "    for i in candidates:\n",
    "        key = i.split(' ')\n",
    "        key.sort()\n",
    "        temp = tuple(key)\n",
    "        if temp not in localCache:\n",
    "            localCache.append(temp)\n",
    "    localCache = [' '.join(item) for item in localCache]\n",
    "    return localCache\n",
    "\n",
    "def prune(df, candidates, frequent, min_sup):\n",
    "    freq_sets = {}\n",
    "    candidates = sortInto(candidates)\n",
    "    for row in df.iterrows():\n",
    "        index, data = row\n",
    "        dataset = set(data)\n",
    "        for i in candidates:\n",
    "            itemset = i.split()\n",
    "            if has_infrequent_subset(i, frequent) == 0:\n",
    "                itemset_set = set(itemset)\n",
    "                diff = itemset_set - dataset\n",
    "                if len(diff) == 0:\n",
    "                    if i not in freq_sets:\n",
    "                        freq_sets[i] = 1\n",
    "                    else:\n",
    "                        freq_sets[i] += 1\n",
    "    for key in list(freq_sets.keys()):\n",
    "        if freq_sets[key] < min_sup:\n",
    "            del freq_sets[key]\n",
    "    return freq_sets\n",
    "\n",
    "def has_infrequent_subset(itemset, frequent):\n",
    "    itemset = itemset.split()\n",
    "    k = len(itemset)\n",
    "    subsets = list(itert.combinations(itemset, k-1))\n",
    "    freq_sets = [set(str(x).split()) for x in list(frequent.keys())]\n",
    "    for i in subsets:\n",
    "        if set(i) not in freq_sets:\n",
    "            return 1\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    min_sup = eval(input('Support Threshold %: '))\n",
    "    start_time = time.time()\n",
    "    apriori(df, min_sup)\n",
    "    run_time = time.time() - start_time\n",
    "    print('\\nExecution Time: ', run_time)\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP-Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, value, weight):\n",
    "        self.value = value\n",
    "        self.weight = weight\n",
    "        self.children = []\n",
    "        \n",
    "    def __repr__(self, level=0):\n",
    "        ret = \"\\t\"*level+repr(self.value)+repr(self.weight)+\"\\n\"\n",
    "        for child in self.children:\n",
    "            ret += child.__repr__(level+1)\n",
    "        return ret\n",
    "    \n",
    "    def isContainedIn(self, obj):\n",
    "        for item in obj.children:\n",
    "            if self.value == item.value:\n",
    "                return item\n",
    "        return False\n",
    "finalList = []\n",
    "\n",
    "\n",
    "def fpg(df, min_sup):\n",
    "    size = len(df.index)\n",
    "    min_sup = (min_sup / 100.0) * size\n",
    "    print('Minimum Support Count: ', int(min_sup))\n",
    "    \n",
    "    # Generate Initial Set of Candidates\n",
    "    candidates = getC1(df)\n",
    "    \n",
    "    # Generate Initial Set of Frequent Itemsets\n",
    "    frequent = {k:v for k, v in list(candidates.items()) if v >= min_sup}\n",
    "    print('Database Scan Complete.')\n",
    "    \n",
    "    # Generate List of Frequent Itemsets in Descending Order\n",
    "    flist = sorted(list(frequent.items()), key=operator.itemgetter(1), reverse=True)\n",
    "    itemsets1 = flist\n",
    "    flist = [i[0] for i in flist]\n",
    "    print('\\nF-List Generated: ', flist)\n",
    "    print('\\nPlease Wait. Scanning Database Second Time.')\n",
    "    df = df.reset_index()\n",
    "        \n",
    "    # Scan Database Again to Find Ordered Frequent Itemlist\n",
    "    ordFreqList = [[] for _ in range(len(df))]\n",
    "    for i, j in df.iterrows():\n",
    "        if i>0 and i % 10000 == 0:\n",
    "            print('Read', i, 'Records.')\n",
    "        row = j.tolist()\n",
    "        for item in flist:\n",
    "            if item in row:\n",
    "                ordFreqList[i].append(item)\n",
    "    print('Database Scan Complete.')\n",
    "    display(itemsets1, 1)\n",
    "    freqItemSets = frequentItems(flist, ordFreqList, min_sup)\n",
    "    total = len(itemsets1) + len(freqItemSets)\n",
    "    dictlist = []\n",
    "    maxi = 0\n",
    "    for key, value in freqItemSets.items():\n",
    "        if len(key) > maxi:\n",
    "            maxi = len(key)\n",
    "        temp = [key,value]\n",
    "        dictlist.append(temp)\n",
    "    for i in range(2, maxi+1):\n",
    "        display(dictlist, i)\n",
    "    \n",
    "    print('\\nFrequent Itemsets Have Been Generated!')\n",
    "    print('No. of Frequent Itemsets Generated: ', total)\n",
    "\n",
    "def getC1(df):\n",
    "    candidates = {}\n",
    "    for index, row in df.iterrows():\n",
    "        data = list(row)\n",
    "        if index>0 and index % 10000 == 0:\n",
    "            print('Read', index, 'Records.')\n",
    "        for item in data:\n",
    "            if item in list(candidates.keys()):\n",
    "                candidates[item] += 1\n",
    "            else:\n",
    "                candidates[item] = 1\n",
    "    return candidates\n",
    "        \n",
    "def display(sets, k):\n",
    "    print('\\n---- Frequent [', k, '] Itemsets ----')\n",
    "    if k == 1:\n",
    "        for i in sets:\n",
    "            key = list(str(i[0]).split())\n",
    "            val = i[1]\n",
    "            print(key, ' - ', val)\n",
    "    else:\n",
    "        for i in sets:\n",
    "            if len(i[0]) == k:\n",
    "                key = list(i[0])\n",
    "                val = i[1]\n",
    "                print(key, ' - ', val)            \n",
    "\n",
    "# Generate the FP Tree\n",
    "def makeTree(flist):\n",
    "    init = Tree([], 0)\n",
    "    for item in flist:\n",
    "        node = init\n",
    "        for elem in item:\n",
    "            newnode = Tree(elem, 1)\n",
    "            index = newnode.isContainedIn(node)\n",
    "            if index:\n",
    "                index.weight += 1\n",
    "                node = index\n",
    "            else:\n",
    "                node = node.children\n",
    "                node.append(newnode)\n",
    "                node = newnode\n",
    "    return init\n",
    "\n",
    "# Generate the Frequent Pattern Base\n",
    "def getPaths(tree, item, lst = []):\n",
    "    node = tree\n",
    "    if node.value == item:\n",
    "        lst.append(node.weight)\n",
    "        temp = []\n",
    "        temp.extend(lst)\n",
    "        temp.pop(0)\n",
    "        finalList.append(temp)\n",
    "        lst.pop()\n",
    "    else:\n",
    "        lst.append(node.value)\n",
    "        for child in node.children:\n",
    "            getPaths(child, item)\n",
    "        lst.pop()\n",
    "\n",
    "# Generate Frequent Itemsets        \n",
    "def frequentItems(items, flist, min_sup):\n",
    "    frequent_patterns = {}\n",
    "    global finalList\n",
    "    tree = makeTree(flist)\n",
    "    for item in items:\n",
    "        getPaths(tree, item)\n",
    "        condPattBase = finalList\n",
    "        localDict = freqPatterns(item, condPattBase, min_sup)\n",
    "        frequent_patterns.update(localDict)\n",
    "        finalList = []\n",
    "    return frequent_patterns\n",
    "        \n",
    "def freqPatterns(item, patterns, min_sup):\n",
    "    globalDict = {}\n",
    "    for pattern in patterns:\n",
    "        localDict = {}\n",
    "        if len(pattern) > 1:\n",
    "            elements = getCombs(pattern[:-1])\n",
    "            for elem in elements:\n",
    "                elem.append(item)\n",
    "                tup = tuple(elem)\n",
    "                elem.pop()\n",
    "                if tup not in list(globalDict.keys()):\n",
    "                    count = getCount(elem, patterns)\n",
    "                    if count >= min_sup:\n",
    "                        localDict[tup] = count\n",
    "        globalDict.update(localDict)\n",
    "    return globalDict\n",
    "\n",
    "def getCombs(x):\n",
    "    n = len(x)\n",
    "    c = [list(comb) for i in range(n) for comb in combinations(x, i + 1)]\n",
    "    return c\n",
    "\n",
    "def getCount(elem, patterns):\n",
    "    support = 0\n",
    "    for pattern in patterns:\n",
    "        flag = 1\n",
    "        for ele in elem:\n",
    "            if ele not in pattern:\n",
    "                flag = 0\n",
    "        if flag == 1:\n",
    "            support += pattern[-1]\n",
    "    return support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    min_sup = eval(input('Support Threshold %: '))\n",
    "    start_time = time.time()\n",
    "    fpg(df, min_sup)\n",
    "    run_time = time.time() - start_time\n",
    "    print('\\nExecution Time: ', run_time)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(df, k):\n",
    "    dataset_size = df.shape[0]\n",
    "    fold_size = int(dataset_size / k)\n",
    "    folds = []\n",
    "    first = 0\n",
    "    last = fold_size\n",
    "    for i in range(k):\n",
    "        folds.append(data_frame.iloc[first:last])\n",
    "        first += fold_size\n",
    "        last += fold_size\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
